{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "from pyspark.sql.types import Row\n",
    "import tensorflow \n",
    "from tensorflow import keras \n",
    "#import tensorflow.keras.experimental.WideDeepModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, concatenate, Embedding, Reshape\n",
    "from tensorflow.keras.layers import Flatten, concatenate, Lambda, Dropout, BatchNormalization\n",
    "#from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2, l1_l2\n",
    "\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def maybe_download(train_data,test_data):\n",
    "    \"\"\"if adult data \"train.csv\" and \"test.csv\" are not in your directory,\n",
    "    download them.\n",
    "    \"\"\"\n",
    "\n",
    "    COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "               \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "               \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "               \"income_bracket\"]\n",
    "\n",
    "    if not os.path.exists(train_data):\n",
    "        print(\"downloading training data...\")\n",
    "        df_train = pd.read_csv(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "            names=COLUMNS, skipinitialspace=True)\n",
    "    else:\n",
    "        df_train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "    if not os.path.exists(test_data):\n",
    "        print(\"downloading testing data...\")\n",
    "        df_test = pd.read_csv(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "            names=COLUMNS, skipinitialspace=True, skiprows=1)\n",
    "    else:\n",
    "        df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def cross_columns(x_cols):\n",
    "    \"\"\"simple helper to build the crossed columns in a pandas dataframe\n",
    "    \"\"\"\n",
    "    crossed_columns = dict()\n",
    "    colnames = ['_'.join(x_c) for x_c in x_cols]\n",
    "    for cname, x_c in zip(colnames, x_cols):\n",
    "        crossed_columns[cname] = x_c\n",
    "    return crossed_columns\n",
    "\n",
    "\n",
    "def val2idx(df, cols):\n",
    "    \"\"\"helper to index categorical columns before embeddings.\n",
    "    \"\"\"\n",
    "    val_types = dict()\n",
    "    for c in cols:\n",
    "        val_types[c] = df[c].unique()\n",
    "\n",
    "    val_to_idx = dict()\n",
    "    for k, v in val_types.items():\n",
    "        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
    "\n",
    "    for k, v in val_to_idx.items():\n",
    "        df[k] = df[k].apply(lambda x: v[x])\n",
    "\n",
    "    unique_vals = dict()\n",
    "    for c in cols:\n",
    "        unique_vals[c] = df[c].nunique()\n",
    "\n",
    "    return df, unique_vals\n",
    "\n",
    "\n",
    "def onehot(x):\n",
    "    return np.array(OneHotEncoder().fit_transform(x).todense())\n",
    "\n",
    "\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, embeddings_regularizer=l2(reg))(inp)\n",
    "\n",
    "\n",
    "def continous_input(name):\n",
    "    inp = Input(shape=(1,), dtype='float32', name=name)\n",
    "    return inp, Reshape((1, 1))(inp)\n",
    "\n",
    "\n",
    "def wide(df_train, df_test, wide_cols, x_cols, target, model_type, method):\n",
    "    \"\"\"Run the wide (linear) model.\n",
    "    Params:\n",
    "    -------\n",
    "    df_train, df_test: train and test datasets\n",
    "    wide_cols   : columns to be used to fit the wide model\n",
    "    x_cols      : columns to be \"crossed\"\n",
    "    target      : the target feature\n",
    "    model_type  : accepts \"wide\" and \"wide_deep\" (or anything that is not\n",
    "    \"wide\"). If \"wide_deep\" the function will build and return the inputs\n",
    "    but NOT run any model.\n",
    "    method      : the fitting method. accepts regression, logistic and multiclass\n",
    "    Returns:\n",
    "    --------\n",
    "    if \"wide\":\n",
    "    print the results obtained on the test set in the terminal.\n",
    "    if \"wide_deep\":\n",
    "    X_train, y_train, X_test, y_test: the inputs required to build wide and deep\n",
    "    \"\"\"\n",
    "\n",
    "    df_train['IS_TRAIN'] = 1\n",
    "    df_test['IS_TRAIN'] = 0\n",
    "    df_wide = pd.concat([df_train, df_test])\n",
    "\n",
    "    # my understanding on how to replicate what layers.crossed_column does. One\n",
    "    # can read here: https://www.tensorflow.org/tutorials/linear.\n",
    "    crossed_columns_d = cross_columns(x_cols)\n",
    "    categorical_columns = list(\n",
    "        df_wide.select_dtypes(include=['object']).columns)\n",
    "\n",
    "    wide_cols += list(crossed_columns_d.keys())\n",
    "\n",
    "    for k, v in crossed_columns_d.items():\n",
    "        df_wide[k] = df_wide[v].apply(lambda x: '-'.join(x), axis=1)\n",
    "\n",
    "    df_wide = df_wide[wide_cols + [target] + ['IS_TRAIN']]\n",
    "\n",
    "    dummy_cols = [\n",
    "        c for c in wide_cols if c in categorical_columns + list(crossed_columns_d.keys())]\n",
    "    df_wide = pd.get_dummies(df_wide, columns=[x for x in dummy_cols])\n",
    "\n",
    "    train = df_wide[df_wide.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
    "    test = df_wide[df_wide.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
    "    assert all(train.columns == test.columns)\n",
    "\n",
    "    cols = [c for c in train.columns if c != target]\n",
    "    X_train = train[cols].values\n",
    "    y_train = train[target].values.reshape(-1, 1)\n",
    "    X_test = test[cols].values\n",
    "    y_test = test[target].values.reshape(-1, 1)\n",
    "    if method == 'multiclass':\n",
    "        y_train = onehot(y_train)\n",
    "        y_test = onehot(y_test)\n",
    "\n",
    "    if model_type == 'wide':\n",
    "\n",
    "        activation, loss, metrics = fit_param[method]\n",
    "        # metrics parameter needs to be passed as a list or dict\n",
    "        if metrics:\n",
    "            metrics = [metrics]\n",
    "\n",
    "        # simply connecting the features to an output layer\n",
    "        wide_inp = Input(shape=(X_train.shape[1],), dtype='float32', name='wide_inp')\n",
    "        w = Dense(y_train.shape[1], activation=activation)(wide_inp)\n",
    "        wide = Model(wide_inp, w)\n",
    "        wide.compile(loss=loss, metrics=metrics, optimizer='Adam')\n",
    "        wide.fit(X_train, y_train, nb_epoch=10, batch_size=64)\n",
    "        results = wide.evaluate(X_test, y_test)\n",
    "\n",
    "        print(\"\\n\", results)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def deep(df_train, df_test, embedding_cols, cont_cols, target, model_type, method):\n",
    "    \"\"\"Run the deep model. Two layers of 100 and 50 neurons. In a decent,\n",
    "    finished code these would be tunable.\n",
    "    Params:\n",
    "    -------\n",
    "    df_train, df_test: train and test datasets\n",
    "    embedding_cols: columns to be passed as embeddings\n",
    "    cont_cols     : numerical columns to be combined with the embeddings\n",
    "    target        : the target feature\n",
    "    model_type    : accepts \"deep\" and \"wide_deep\" (or anything that is not\n",
    "    \"wide\"). If \"wide_deep\" the function will build and returns the inputs\n",
    "    but NOT run any model\n",
    "    method        : the fitting method. accepts regression, logistic and multiclass\n",
    "    Returns:\n",
    "    --------\n",
    "    if \"deep\":\n",
    "    print the results obtained on the test set in the terminal.\n",
    "    if \"wide_deep\":\n",
    "    X_train, y_train, X_test, y_test: the inputs required to build wide and deep\n",
    "    inp_embed, inp_layer: the embedding layers and the input tensors for Model()\n",
    "    \"\"\"\n",
    "\n",
    "    df_train['IS_TRAIN'] = 1\n",
    "    df_test['IS_TRAIN'] = 0\n",
    "    df_deep = pd.concat([df_train, df_test])\n",
    "\n",
    "    deep_cols = embedding_cols + cont_cols\n",
    "    df_deep = df_deep[deep_cols + [target,'IS_TRAIN']]\n",
    "    scaler = StandardScaler()\n",
    "    df_deep[cont_cols] = pd.DataFrame(scaler.fit_transform(df_train[cont_cols]),\n",
    "        columns=cont_cols)\n",
    "    df_deep, unique_vals = val2idx(df_deep, embedding_cols)\n",
    "\n",
    "    train = df_deep[df_deep.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
    "    test = df_deep[df_deep.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
    "\n",
    "    embeddings_tensors = []\n",
    "    n_factors = 8\n",
    "    reg = 1e-3\n",
    "    for ec in embedding_cols:\n",
    "        layer_name = ec + '_inp'\n",
    "        t_inp, t_build = embedding_input(\n",
    "            layer_name, unique_vals[ec], n_factors, reg)\n",
    "        embeddings_tensors.append((t_inp, t_build))\n",
    "        del(t_inp, t_build)\n",
    "\n",
    "    continuous_tensors = []\n",
    "    for cc in cont_cols:\n",
    "        layer_name = cc + '_in'\n",
    "        t_inp, t_build = continous_input(layer_name)\n",
    "        continuous_tensors.append((t_inp, t_build))\n",
    "        del(t_inp, t_build)\n",
    "\n",
    "    X_train = [train[c] for c in deep_cols]\n",
    "    y_train = np.array(train[target].values).reshape(-1, 1)\n",
    "    X_test = [test[c] for c in deep_cols]\n",
    "    y_test = np.array(test[target].values).reshape(-1, 1)\n",
    "\n",
    "    if method == 'multiclass':\n",
    "        y_train = onehot(y_train)\n",
    "        y_test = onehot(y_test)\n",
    "\n",
    "    inp_layer =  [et[0] for et in embeddings_tensors]\n",
    "    inp_layer += [ct[0] for ct in continuous_tensors]\n",
    "    inp_embed =  [et[1] for et in embeddings_tensors]\n",
    "    inp_embed += [ct[1] for ct in continuous_tensors]\n",
    "\n",
    "    if model_type == 'deep':\n",
    "\n",
    "        activation, loss, metrics = fit_param[method]\n",
    "        if metrics:\n",
    "            metrics = [metrics]\n",
    "\n",
    "        d = concatenate(inp_embed)\n",
    "        d = Flatten()(d)\n",
    "        d = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "        d = Dropout(0.5)(d) # Dropout don't seem to help in this model\n",
    "        d = Dense(50, activation='relu')(d)\n",
    "        d = Dropout(0.5)(d) # Dropout don't seem to help in this model\n",
    "        d = Dense(y_train.shape[1], activation=activation)(d)\n",
    "        deep = Model(inp_layer, d)\n",
    "        deep.compile(loss=loss, metrics=metrics, optimizer='Adam')\n",
    "        deep.fit(X_train, y_train, batch_size=64, nb_epoch=10)\n",
    "        results = deep.evaluate(X_test, y_test)\n",
    "\n",
    "        print(\"\\n\", results)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, inp_embed, inp_layer\n",
    "\n",
    "\n",
    "def wide_deep(df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols, method):\n",
    "    \"\"\"Run the wide and deep model. Parameters are the same as those for the\n",
    "    wide and deep functions\n",
    "    \"\"\"\n",
    "\n",
    "    # Default model_type is \"wide_deep\"\n",
    "    X_train_wide, y_train_wide, X_test_wide, y_test_wide = \\\n",
    "        wide(df_train, df_test, wide_cols, x_cols, target, model_type, method)\n",
    "\n",
    "    X_train_deep, y_train_deep, X_test_deep, y_test_deep, deep_inp_embed, deep_inp_layer = \\\n",
    "        deep(df_train, df_test, embedding_cols,cont_cols, target, model_type, method)\n",
    "\n",
    "    X_tr_wd = [X_train_wide] + X_train_deep\n",
    "    Y_tr_wd = y_train_deep  # wide or deep is the same here\n",
    "    X_te_wd = [X_test_wide] + X_test_deep\n",
    "    Y_te_wd = y_test_deep  # wide or deep is the same here\n",
    "\n",
    "    activation, loss, metrics = fit_param[method]\n",
    "    if metrics: metrics = [metrics]\n",
    "\n",
    "    # WIDE\n",
    "    w = Input(shape=(X_train_wide.shape[1],), dtype='float32', name='wide')\n",
    "\n",
    "    # DEEP: the output of the 50 neurons layer will be the deep-side input\n",
    "    d = concatenate(deep_inp_embed)\n",
    "    d = Flatten()(d)\n",
    "    d = Dense(50, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(d)\n",
    "    d = Dropout(0.5)(d)\n",
    "    d = Dense(20, activation='relu', name='deep')(d)\n",
    "    d = Dropout(0.5)(d)\n",
    "\n",
    "    # WIDE + DEEP\n",
    "    wd_inp = concatenate([w, d])\n",
    "    wd_out = Dense(Y_tr_wd.shape[1], activation=activation, name='wide_deep')(wd_inp)\n",
    "    wide_deep = Model(inputs=[w] + deep_inp_layer, outputs=wd_out)\n",
    "    wide_deep.compile(optimizer='Adam', loss=loss, metrics=metrics)\n",
    "    wide_deep.fit(X_tr_wd, Y_tr_wd, epochs=5, batch_size=128)\n",
    "\n",
    "    # Maybe you want to schedule a second search with lower learning rate\n",
    "    wide_deep.optimizer.lr = 0.0001\n",
    "    wide_deep.fit(X_tr_wd, Y_tr_wd, epochs=5, batch_size=128)\n",
    "\n",
    "    results = wide_deep.evaluate(X_te_wd, Y_te_wd)\n",
    "\n",
    "    print(\"\\n\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--method\", type=str, default=\"logistic\",help=\"fitting method\")\n",
    "    ap.add_argument(\"--model_type\", type=str, default=\"wide_deep\",help=\"wide, deep or both\")\n",
    "    ap.add_argument(\"--train_data\", type=str, default=\"train.csv\")\n",
    "    ap.add_argument(\"--test_data\", type=str, default=\"test.csv\")\n",
    "    args = vars(ap.parse_args())\n",
    "    method = args[\"method\"]\n",
    "    model_type = args['model_type']\n",
    "    train_data = args['train_data']\n",
    "    test_data = args['test_data']\n",
    "\n",
    "    fit_param = dict()\n",
    "    fit_param['logistic']   = ('sigmoid', 'binary_crossentropy', 'accuracy')\n",
    "    fit_param['regression'] = (None, 'mse', None)\n",
    "    fit_param['multiclass'] = ('softmax', 'categorical_crossentropy', 'accuracy')\n",
    "\n",
    "    # df_train, df_test = maybe_download(train_data, test_data)\n",
    "    df_train = pd.read_csv(\"train.csv\")\n",
    "    df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "    # Add a feature to illustrate the logistic regression example\n",
    "    df_train['income_label'] = (\n",
    "        df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "    df_test['income_label'] = (\n",
    "        df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "    # Add a feature to illustrate multiclass classification\n",
    "    age_groups = [0, 25, 65, 90]\n",
    "    age_labels = range(len(age_groups) - 1)\n",
    "    df_train['age_group'] = pd.cut(\n",
    "        df_train['age'], age_groups, labels=age_labels)\n",
    "    df_test['age_group'] = pd.cut(\n",
    "        df_test['age'], age_groups, labels=age_labels)\n",
    "\n",
    "    # columns for wide model\n",
    "    wide_cols = ['workclass', 'education', 'marital_status', 'occupation',\n",
    "        'relationship', 'race', 'gender', 'native_country', 'age_group']\n",
    "    x_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "\n",
    "    # columns for deep model\n",
    "    embedding_cols = ['workclass', 'education', 'marital_status', 'occupation',\n",
    "                      'relationship', 'race', 'gender', 'native_country']\n",
    "    cont_cols = ['age', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "\n",
    "    # target for logistic\n",
    "    target = 'income_label'\n",
    "\n",
    "    # # A set-up for multiclass classification would be:\n",
    "    # # change method to multiclass\n",
    "    # wide_cols = [\"gender\", \"native_country\", \"education\", \"occupation\", \"workclass\",\n",
    "    #              \"relationship\"]\n",
    "    # x_cols = (['education', 'occupation'], ['native_country', 'occupation'])\n",
    "\n",
    "    # # columns for deep model\n",
    "    # embedding_cols = ['education', 'relationship', 'workclass', 'occupation',\n",
    "    #                   'native_country']\n",
    "    # cont_cols = [\"hours_per_week\"]\n",
    "\n",
    "    # # target\n",
    "    # target = 'age_group'\n",
    "\n",
    "    if model_type == 'wide':\n",
    "        wide(df_train, df_test, wide_cols, x_cols, target, model_type, method)\n",
    "    elif model_type == 'deep':\n",
    "        deep(df_train, df_test, embedding_cols, cont_cols, target, model_type, method)\n",
    "    else:\n",
    "        wide_deep(df_train, df_test, wide_cols, x_cols, embedding_cols, cont_cols, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wide and deep of using Tensorflow 2.4 \n",
    "linear_model = keras.experimental.LinearModel()\n",
    "linear_model.compile('adagrad', 'mse')\n",
    "linear_model.fit(linear_inputs, y, epochs)\n",
    "dnn_model = keras.Sequential([keras.layers.Dense(units=1)])\n",
    "dnn_model.compile('rmsprop', 'mse')\n",
    "dnn_model.fit(dnn_inputs, y, epochs)\n",
    "combined_model = keras.experimental.WideDeepModel(linear_model, dnn_model)\n",
    "combined_model.compile(optimizer=['sgd', 'adam'], 'mse', ['mse'])\n",
    "combined_model.fit([linear_inputs, dnn_inputs], y, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernel_args": {
   "archives": [
    "/user/meizhu/zips/holidays.zip"
   ],
   "conf": {
    "spark.pyspark.python": "/opt/python/bin/python3.6"
   },
   "version": "current"
  },
  "kernelspec": {
   "display_name": "PySpark 3 (Beta)",
   "language": "",
   "name": "pysparkkernel3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "ipython2",
   "version": "1.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
